This repo is created to learn about nlp related tasks with the help of transformers(specially using ðŸ¤— transformers).

The Transformer architecture in paper <b>[Attention Is All You Need](https://arxiv.org/abs/1706.03762)<b> was originally designed for translation related tasks. 

Below are some resources to learn about the original architecture:
1. [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
2. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
3. [Transformers from Scratch](https://e2eml.school/transformers.html)
4. [How Transformers work in deep learning and NLP: an intuitive introduction](https://theaisummer.com/transformer/)
5. [Getting meaning from text](https://peltarion.com/blog/data-science/self-attention-video)
6. [TRANSFORMERS FROM SCRATCH](http://peterbloem.nl/blog/transformers)
7. [Transformer Recipe](https://github.com/dair-ai/Transformers-Recipe)
8. [Pytorch implementation of original paper Attention Is All You Need](https://github.com/gordicaleksa/pytorch-original-transformer)



We will start with [huggingface course](https://huggingface.co/course/chapter1/1) where we will learn about transformer, how it works, how to use huggingface already trained tranformer models for common nlp tasks like sentiment analysis, quetion answering, text classification, NER (named entity recognition), generating a new sentense from the input text etc.


